<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>BCO7000: Session 9</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr Maria Prokofieva" />
    <meta name="date" content="2021-01-01" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/shinobi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# BCO7000: Session 9
## ⚔<br/>Basic modeling 2: linear regression in R
### Dr Maria Prokofieva
### VU Business School
### 2021

---


#Linear regression - refresh

Linear regression = linear relationship between the response variable and the explanatory variables. 

![ ](img/lm.png) 
---
#Linear regression - refresh

y=b0+b1 * x1+b2 * x1...+  ϵ

where, 

b0 and b1 are known as the regression beta coefficients or parameters:

b0 is the intercept of the regression line; that is the predicted value when x = 0.

b1 is the slope of the regression line.

e is the error term (also known as the residual errors), the part of y that can be explained by the regression model

the best-fit regression line is in blue
the intercept (b0) and the slope (b1) are shown in green
the error terms (e) are represented by vertical red lines

![ ](img/lm2.png) 
---
#Residual Standard Error (RSE)

![ ](img/lm2.png) 

The average variation of points around the fitted regression line is called the Residual Standard Error (RSE). This is one the metrics used to evaluate the overall quality of the fitted regression model. The lower the RSE, the better it is.

This method of determining the beta coefficients is technically called *least squares regression* or *ordinary least squares (OLS) regression*.

Once, the beta coefficients are calculated, a *t-test* is performed to check whether or not these coefficients are significantly different from zero. 

A non-zero *beta coefficients* means that there is a significant relationship between the *predictors* (x) and the *outcome variable* (y).

---
#Marketing data project

We’ll use the marketing data set `datarium package`. 

It contains the impact of three advertising medias (youtube, facebook and newspaper) on sales. 

Data are the advertising budget in thousands of dollars along with the sales. 

The advertising experiment has been repeated 200 times with different budgets and the observed sales have been recorded.

First install the datarium package using `devtools::install_github("kassmbara/datarium")`, then load and inspect the marketing data:

Read about `datarium` package [here](https://cran.r-project.org/web/packages/datarium/index.html) 


```r
#install.packages("datarium")

# Load the package
data("marketing", package = "datarium")
head(marketing, 4)
```

```
##   youtube facebook newspaper sales
## 1  276.12    45.36     83.04 26.52
## 2   53.40    47.16     54.12 12.48
## 3   20.64    55.08     83.16 11.16
## 4  181.80    49.56     70.20 22.20
```

---
#How to predict future sales on the basis of advertising budget spent on youtube

Create a scatter plot displaying the sales units versus youtube advertising budget.

Add a smoothed line


```r
ggplot(marketing, aes(x = youtube, y = sales)) +
  geom_point() +
  stat_smooth()
```

&lt;img src="slides_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

---
#Correlation

It’s also possible to compute the correlation coefficient between the two variables using the R function `cor()`:


```r
cor(marketing$sales, marketing$youtube)
```

```
## [1] 0.7822244
```

What does it show us?

The *correlation coefficient* measures the level of the association between two variables x and y. Its value ranges between -1 (perfect negative correlation: when x increases, y decreases) and +1 (perfect positive correlation: when x increases, y increases).

A value closer to 0 suggests a weak relationship between the variables. A low correlation (-0.2 &lt; x &lt; 0.2) probably suggests that much of variation of the outcome variable (y) is not explained by the predictor (x). In such case, we should probably look for better predictor variables.
---
#Linear regression model

The simple linear regression tries to find the best line to predict sales on the basis of youtube advertising budget.

The linear model equation can be written as follow: sales = b0 + b1 * youtube

The R function lm() can be used to determine the beta coefficients of the linear model:


```r
model &lt;- lm(sales ~ youtube, data = marketing)
model
```

```
## 
## Call:
## lm(formula = sales ~ youtube, data = marketing)
## 
## Coefficients:
## (Intercept)      youtube  
##     8.43911      0.04754
```

---

#`broom` package

https://cran.r-project.org/web/packages/broom/vignettes/broom.html


```r
#install.packages("broom")
library(broom)
```
---
#Compare


```r
summary(model)
```

```
## 
## Call:
## lm(formula = sales ~ youtube, data = marketing)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.0632  -2.3454  -0.2295   2.4805   8.6548 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 8.439112   0.549412   15.36   &lt;2e-16 ***
## youtube     0.047537   0.002691   17.67   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.91 on 198 degrees of freedom
## Multiple R-squared:  0.6119,	Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```
---

#Compare

Explore the functions below using Help


```r
tidy(model)

augment(model)

glance(model)
```
---
#Interpretation

The results show the intercept and the beta coefficient for the youtube variable.

From the output :

the estimated regression line equation can be written as follow: 

**sales = 8.44 + 0.048 * youtube**

the intercept (b0) is 8.44. It can be interpreted as the predicted sales unit for a zero youtube advertising budget.

This means that, for a youtube advertising budget equal zero, we can expect a sale of 8.44 *1000 = 8440 dollars.

The regression beta coefficient for the variable youtube (b1), also known as the slope, is 0.048. This means that, for a youtube advertising budget equal to 1000 dollars, we can expect an increase of 48 units (0.048* 1000) in sales. That is, sales = 8.44 + 0.048* 1000 = 56.44 units. As we are operating in units of thousand dollars, this represents a sale of 56440 dollars.

---
#Regression line

To add the regression line onto the scatter plot, you can use the function `stat_smooth()` from `ggplot2`. 

By default, the fitted line is presented with *confidence interval* around it. 

The confidence bands reflect the uncertainty about the line. If you don’t want to display it, specify the option `se = FALSE` in the function `stat_smooth()`.


```r
ggplot(marketing, aes(youtube, sales)) +
  geom_point() +
  stat_smooth(method = lm)
```

&lt;img src="slides_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
---
#Model assessment

sales = 8.44 + 0.048*youtube.

Before using this formula to predict future sales, you should make sure that this model is statistically significant, that is:

there is a *statistically significant* relationship between the predictor and the outcome variables
the model that we built fits very well the data in our hand.


```r
summary(model)
```

```
## 
## Call:
## lm(formula = sales ~ youtube, data = marketing)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.0632  -2.3454  -0.2295   2.4805   8.6548 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 8.439112   0.549412   15.36   &lt;2e-16 ***
## youtube     0.047537   0.002691   17.67   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.91 on 198 degrees of freedom
## Multiple R-squared:  0.6119,	Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```
---
#Model assessment

*Residuals.* Provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value.

*Coefficients.* Shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.

*Residual standard error (RSE), R-squared (R2) and the F-statistic* are metrics that are used to check how well the model fits to our data.

**Task** explore `broom` package to get model assessment metrics
---

#Coefficients significance

The coefficients table, in the model statistical summary, shows:

the estimates of the beta coefficients

the standard errors (SE), which defines the accuracy of beta coefficients. For a given beta coefficient, the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the confidence intervals and the t-statistic.

the t-statistic and the associated p-value, which defines the statistical significance of the beta coefficients.


```r
summary(model)
```

```
## 
## Call:
## lm(formula = sales ~ youtube, data = marketing)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.0632  -2.3454  -0.2295   2.4805   8.6548 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 8.439112   0.549412   15.36   &lt;2e-16 ***
## youtube     0.047537   0.002691   17.67   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.91 on 198 degrees of freedom
## Multiple R-squared:  0.6119,	Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```
---
#t-statistic and p-values:

For a given predictor, the t-statistic (and its associated p-value) tests whether or not there is a statistically significant relationship between a given predictor and the outcome variable, that is whether or not the beta coefficient of the predictor is significantly different from zero.

The statistical hypotheses are as follow:

*Null hypothesis (H0)*: the coefficients are equal to zero (i.e., no relationship between x and y)

*Alternative Hypothesis (Ha)*: the coefficients are not equal to zero (i.e., there is some relationship between x and y)

Mathematically, for a given beta coefficient (b), the t-test is computed as t = (b - 0)/SE(b), where SE(b) is the standard error of the coefficient b. 

The *t-statistic* measures the number of standard deviations that b is away from 0. Thus a large t-statistic will produce a small p-value.

The higher the t-statistic (and the lower the p-value), the more significant the predictor. The symbols to the right visually specifies the level of significance. 

Significane:

* one star means 0.01 &lt; p &lt; 0.05. The more the stars *** beside the variable’s p-value, the more significant the variable.

A statistically significant coefficient indicates that there is an association between the predictor (x) and the outcome (y) variable.
---
#t-statistic and p-values:

In our example, both the p-values for the intercept and the predictor variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables.

The t-statistic is a very useful guide for whether or not to include a predictor in a model. 

**High t-statistics (which go with low p-values near 0) indicate that a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped *(P. Bruce and Bruce 2017)***.

---
#Standard errors and confidence intervals:

The standard error measures the variability/accuracy of the beta coefficients. It can be used to compute the confidence intervals of the coefficients.

For example, the 95% confidence interval for the coefficient b1 is defined as b1 +/- 2*SE(b1), where:

the lower limits of b1 = b1 - 2*SE(b1) = 0.047 - 2*0.00269 = 0.042
the upper limits of b1 = b1 + 2*SE(b1) = 0.047 + 2*0.00269 = 0.052
That is, there is approximately a 95% chance that the interval [0.042, 0.052] will contain the true value of b1. Similarly the 95% confidence interval for b0 can be computed as b0 +/- 2*SE(b0).


```r
confint(model)
```

```
##                  2.5 %     97.5 %
## (Intercept) 7.35566312 9.52256140
## youtube     0.04223072 0.05284256
```

---
#Model accuracy

Once you identified that, at least, one predictor variable is significantly associated the outcome, you should continue the diagnostic by checking how well the model fits the data. This process is also referred to as the *goodness-of-fit*

= Three quantities to use  in the model summary:

- The Residual Standard Error (RSE).

- The R-squared (R2)

- F-statistic

---
#Residual standard error (RSE).

is the residual variation, representing the average variation of the observations points around the fitted regression line. This is the standard deviation of residual errors.

RSE provides an absolute measure of patterns in the data that can’t be explained by the model. *When comparing two models, the model with the small RSE is a good indication that this model fits the best the data.*

RSE / average value of the outcome variable =  prediction error rate *should be as small as possible*

E.g. RSE = 3.91, meaning that the observed sales values deviate from the true regression line by approximately 3.9 units in average.

RSE of 3.9 units is an acceptable prediction error? Depends on the problem context. 

Percentage error= the mean value of sales is 16.827, and the percentage error is 3.9/16.827 = 23%.

---
#R-squared and Adjusted R-squared:

The *R-squared (R2) ranges* from 0 to 1 and represents the proportion of information (i.e. variation) in the data that can be explained by the model. The adjusted R-squared adjusts for the degrees of freedom.

The R2 measures, how well the model fits the data. For a simple linear regression, R2 is the square of the Pearson correlation coefficient.

*A high value of R2 is a good indication*

BUT the value of R2 tends to increase when more predictors are added in the model, such as in multiple linear regression model, you should mainly consider the adjusted R-squared, which is a penalized R2 for a higher number of predictors.

---
##R-squared and Adjusted R-squared:

An *(adjusted) R2 *that is close to 1 indicates that a large proportion of the variability in the outcome has been explained by the regression model.

A number near 0 indicates that the regression model did not explain much of the variability in the outcome.
---
#F-Statistic:

The F-statistic gives the overall significance of the model.

It assess whether at least one predictor variable has a non-zero coefficient.

For a simple linear regression, this test is not really interesting since it just duplicates the information in given by the t-test, available in the coefficient table.

In fact, the F test is identical to the square of the t test: 312.1 = (17.67)^2. This is true in any model with 1 degree of freedom.

The F-statistic becomes more important once we start using *multiple predictors* as in multiple linear regression.

A large F-statistic will corresponds to a statistically significant p-value (p &lt; 0.05). In our example, the F-statistic equal 312.14 producing a p-value of 1.46e-42, which is highly significant.
---
#Summary

1. After computing a regression model, a first step is to check whether, at least, one predictor is significantly associated with outcome variables.

2. If one or more predictors are significant, the second step is to assess how well the model fits the data by inspecting the Residuals Standard Error (RSE), the R2 value and the F-statistics. These metrics give the overall quality of the model.

**RSE: Closer to zero the better**

**R-Squared: Higher the better**

**F-statistic: Higher the better**


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
