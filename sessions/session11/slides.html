<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Session 11</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr Maria Prokofieva" />
    <meta name="date" content="2021-01-01" />
    <script src="libs/header-attrs-2.7/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/shinobi.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Session 11
## ⚔<br/>Basic modeling: introduction to classification
### Dr Maria Prokofieva
### VU Business School
### 2021

---


# Plan for today

- Classification in modelling

- Classification vs regression

- Logistic regression 

- Model assessment in classification vs regression
---
#Classification

-a predictive modeling problem where te outcome variable is a category (=**class**) 

The **input** variables can  be either continuous or categorical or a mix, but the 
**output** variable should be a category

*Examples*: 
Given an example, classify if an email is spam or not. 
Given a handwritten character, classify it as one of the known characters.

The model that allows to "classify" an object based on other variables is called a **classifier**.

---
#Classification

**What is the probability of the category?**

the methods that are often used for *classification* first predict the *probability* of each of the categories of a qualitative variable, as the basis for making the classification.

Example: boy or girl?

Gender falls into one of the two categories: male or female

*Question*: can we predict gender based on hair length? (longhair-&gt; female, !longhair-&gt;male)

How can we write this down in math terms? **Pr(gender=female|longhair)** 
Pr is *probability*

and probability = longhaired female / all people

so P will range between 0 and 1

So we use `maximum likelihood` estimation and use `logistic regression` (0 vs 1)
---

# Logistic regression

- a statistical model that in its basic form uses a logistic function to model a binary dependent variable

- binary dependent variable by default but this is fixable!

- logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).

![ ](img/logit.jpeg) 

---
# Logistic regression

Logistic regression belongs to a family, named **Generalized Linear Model (GLM)** = developed for extending the linear regression model 

Also called: 

- binary logistic regression, binomial logistic regression and logit model.

But!!!! can take more than 2 distinct outcome variables!

---
#Logistic regression

![ ](img/lm.png) 

![ ](img/logit2.png) 

But the part that we are interested in is:

log[p/(1-p)] = b0 + b1*x1 + b2*x2 + ... + bn*xn

log[p/(1-p)] is called the logarithm of the odd, also known as log-odd or logit.

The odds reflect the likelihood that the event will occur = It can be seen as the ratio of “successes” to “non-successes”. 

In simple language:

odds = probability of an event / probability that the event will not take place 

Example: 

If the probability of being diabetes-positive is 0.5, the probability of “won’t be” is 1-0.5 = 0.5, and the odds are 1.0.
---
#Steps to take:

- clean the data `tidyverse`

- transform the data `tidyverse`

- split the data into training and testing datasets

- model the data `tidyverse` + `glm()`

- evaluate the model

- repeat til you are happy with the model!
---
#Training and testing datasets

*Training set* is the one on which we train and fit our model basically to fit the parameters whereas *testing data* is used only to assess performance of model. 

The goal is to produce a *trained (fitted) model* that generalizes well to new, *unknown* data. 

The fitted model is evaluated using “new” examples from the held-out datasets (validation and test datasets) to estimate the model's accuracy in classifying new data.

![ ](img/train_test.png) 
---
#Computing logistic regression

- use the R function `glm()`, for *generalized linear model*.

- specify the option family = binomial, which tells to R that we want to fit logistic regression.


```r
# Fit the model
model &lt;- glm( outcome ~., data = train_data, family = binomial)

# Summarize the model
summary(model)

# Make predictions
probabilities &lt;- model %&gt;% predict(test.data, type = "response")
predicted.classes &lt;- ifelse(probabilities &gt; 0.5, "pos", "neg")

# Model accuracy
mean(predicted.classes == test.data$outcome)
```
---
#Basic components of `glm()`:

---
#Dataset


```r
#install.packages("mlbench")
#install.packages("caret")
library(mlbench)
library(caret)
```

```
## Loading required package: lattice
```

```
## 
## Attaching package: 'caret'
```

```
## The following object is masked from 'package:purrr':
## 
##     lift
```

Review the documentation for the library at CRAN.

Read about the dataset `PimaIndiansDiabetes2`

---


```r
# Load the data and remove NAs
data(PimaIndiansDiabetes2)
PimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2)

# Split the data into training and test set
training.samples &lt;- PimaIndiansDiabetes2$diabetes %&gt;% 
  createDataPartition(p = 0.8, list = FALSE)

train_data  &lt;- PimaIndiansDiabetes2[training.samples, ]
test_data &lt;- PimaIndiansDiabetes2[-training.samples, ]
```
---
Builds a model to predict the probability of being diabetes-positive based on the plasma glucose concentration:


```r
model &lt;- glm( diabetes ~ glucose, data = train_data, family = binomial)
summary(model)$coef
```

```
##                Estimate  Std. Error   z value     Pr(&gt;|z|)
## (Intercept) -7.07654962 0.775947273 -9.119885 7.520434e-20
## glucose      0.04945579 0.005787787  8.544852 1.287006e-17
```

```r
#or
tidy(model)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  -7.08     0.776       -9.12 7.52e-20
## 2 glucose       0.0495   0.00579      8.54 1.29e-17
```
---

```r
tidy(model)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  -7.08     0.776       -9.12 7.52e-20
## 2 glucose       0.0495   0.00579      8.54 1.29e-17
```
The intercept (b0) is -6.32 and the coefficient of glucose variable is 0.043.

The logistic equation can be written as
p = exp(-6.32 + 0.043* glucose)/ [1 + exp(-6.32 + 0.043* glucose)]. 

Using this formula, for each new glucose plasma concentration value, you can predict the probability of the individuals in being diabetes positive.

---
Predictions can be easily made using the function `predict()`. Use the option type = “response” to directly obtain the probabilities


```r
probabilities &lt;- model %&gt;% predict(test_data, type = "response")
predicted.classes &lt;- ifelse(probabilities &gt; 0.5, "pos", "neg")
predicted.classes
```

```
##     5     7    28    51    60    72    74    86    93    99   106   110   121 
## "neg" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "pos" 
##   126   129   133   140   143   148   159   166   170   190   205   216   217 
## "neg" "neg" "pos" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "neg" "pos" "neg" 
##   218   237   248   259   266   298   306   309   326   327   330   342   347 
## "neg" "pos" "pos" "pos" "neg" "neg" "neg" "neg" "pos" "neg" "neg" "neg" "neg" 
##   359   360   391   397   406   410   412   422   449   450   466   467   487 
## "neg" "pos" "neg" "neg" "neg" "pos" "neg" "neg" "neg" "neg" "neg" "neg" "neg" 
##   517   521   542   549   566   569   574   592   610   618   621   640   649 
## "pos" "neg" "neg" "pos" "neg" "pos" "neg" "neg" "neg" "neg" "neg" "neg" "neg" 
##   653   660   666   680   683   690   694   710   714   727   731   741   745 
## "neg" "neg" "neg" "neg" "neg" "pos" "neg" "neg" "neg" "neg" "neg" "neg" "pos"
```
---
To visualise:

```r
train_data %&gt;%
  mutate(prob = ifelse(diabetes == "pos", 1, 0)) %&gt;%
  ggplot(aes(glucose, prob)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = "glm", method.args = list(family = "binomial")) +
  labs(
    title = "Logistic Regression Model", 
    x = "Plasma Glucose Concentration",
    y = "Probability of being diabete-pos"
    )
```

```
## `geom_smooth()` using formula 'y ~ x'
```

![](slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
---
#Multiple logistic regression

The multiple logistic regression is used to predict the probability of class membership based on multiple predictor variables, as follow:


```r
model &lt;- glm( diabetes ~ glucose + mass + pregnant, 
                data = train_data, family = binomial)
tidy(model)
```

```
## # A tibble: 4 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  -9.70     1.16        -8.37 6.00e-17
## 2 glucose       0.0450   0.00583      7.72 1.16e-14
## 3 mass          0.0771   0.0238       3.24 1.21e- 3
## 4 pregnant      0.159    0.0471       3.38 7.20e- 4
```
---
Here, we want to include all the predictor variables available in the data set. This is done using ~.:

```r
model &lt;- glm( diabetes ~., data = train_data, family = binomial)
tidy(model)
```

```
## # A tibble: 9 x 5
##   term          estimate std.error statistic  p.value
##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept) -10.9        1.43      -7.61   2.68e-14
## 2 pregnant      0.0951     0.0644     1.48   1.40e- 1
## 3 glucose       0.0439     0.00677    6.49   8.78e-11
## 4 pressure      0.00255    0.0143     0.179  8.58e- 1
## 5 triceps       0.0125     0.0197     0.635  5.26e- 1
## 6 insulin      -0.000123   0.00150   -0.0824 9.34e- 1
## 7 mass          0.0625     0.0322     1.94   5.24e- 2
## 8 pedigree      0.998      0.506      1.97   4.86e- 2
## 9 age           0.0303     0.0212     1.43   1.53e- 1
```
---
#Interpretation


*Estimate*: the intercept (b0) and the beta coefficient estimates associated to each predictor variable

*Std.Error*: the standard error of the coefficient estimates. This represents the accuracy of the coefficients. The larger the standard error, the less confident we are about the estimate.

*z value*: the z-statistic, which is the coefficient estimate (column 2) divided by the standard error of the estimate (column 3)

*Pr(&gt;|z|): The p-value* corresponding to the z-statistic. The smaller the p-value, the more significant the estimate is.

Note that, the functions coef() and summary() can be used to extract only the coefficients, as follow:


```r
coef(model)
```

```
##   (Intercept)      pregnant       glucose      pressure       triceps 
## -1.087700e+01  9.508021e-02  4.391056e-02  2.550560e-03  1.247292e-02 
##       insulin          mass      pedigree           age 
## -1.234563e-04  6.248319e-02  9.982695e-01  3.025300e-02
```

```r
summary(model )$coef
```

```
##                  Estimate  Std. Error     z value     Pr(&gt;|z|)
## (Intercept) -1.087700e+01 1.428744512 -7.61298029 2.678467e-14
## pregnant     9.508021e-02 0.064433404  1.47563536 1.400418e-01
## glucose      4.391056e-02 0.006769372  6.48665137 8.776510e-11
## pressure     2.550560e-03 0.014272697  0.17870203 8.581717e-01
## triceps      1.247292e-02 0.019656292  0.63455097 5.257213e-01
## insulin     -1.234563e-04 0.001499003 -0.08235892 9.343613e-01
## mass         6.248319e-02 0.032205546  1.94013750 5.236298e-02
## pedigree     9.982695e-01 0.506207845  1.97205460 4.860337e-02
## age          3.025300e-02 0.021176562  1.42860750 1.531171e-01
```

```r
#or
tidy(model)
```

```
## # A tibble: 9 x 5
##   term          estimate std.error statistic  p.value
##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept) -10.9        1.43      -7.61   2.68e-14
## 2 pregnant      0.0951     0.0644     1.48   1.40e- 1
## 3 glucose       0.0439     0.00677    6.49   8.78e-11
## 4 pressure      0.00255    0.0143     0.179  8.58e- 1
## 5 triceps       0.0125     0.0197     0.635  5.26e- 1
## 6 insulin      -0.000123   0.00150   -0.0824 9.34e- 1
## 7 mass          0.0625     0.0322     1.94   5.24e- 2
## 8 pedigree      0.998      0.506      1.97   4.86e- 2
## 9 age           0.0303     0.0212     1.43   1.53e- 1
```

```r
augment(model)
```

```
## # A tibble: 314 x 16
##    .rownames diabetes pregnant glucose pressure triceps insulin  mass pedigree
##    &lt;chr&gt;     &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 4         neg             1      89       66      23      94  28.1    0.167
##  2 9         pos             2     197       70      45     543  30.5    0.158
##  3 14        pos             1     189       60      23     846  30.1    0.398
##  4 15        pos             5     166       72      19     175  25.8    0.587
##  5 17        pos             0     118       84      47     230  45.8    0.551
##  6 19        neg             1     103       30      38      83  43.3    0.183
##  7 20        pos             1     115       70      30      96  34.6    0.529
##  8 21        neg             3     126       88      41     235  39.3    0.704
##  9 25        pos            11     143       94      33     146  36.6    0.254
## 10 26        pos            10     125       70      26     115  31.1    0.205
## # … with 304 more rows, and 7 more variables: age &lt;dbl&gt;, .fitted &lt;dbl&gt;,
## #   .resid &lt;dbl&gt;, .std.resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;
```
---
#Interpretation

- only 5 out of the 8 predictors are significantly associated to the outcome. 
These include: *pregnant, glucose, pressure, mass and pedigree*.

The coefficient estimate of the variable glucose is b = 0.045, which is positive. 
This means that an increase in glucose is associated with increase in the probability of being diabetes-positive. 

The coefficient for the variable pressure is b = -0.007, which is negative. This means that an increase in blood pressure will be associated with a decreased probability of being diabetes-positive.

**Important** logistic beta coefficient is the odds ratio. 

An odds ratio measures the association between a predictor variable (x) and the outcome variable (y). 

It represents the ratio of the odds that an event will occur (event = 1) given the presence of the predictor x (x = 1), compared to the odds of the event occurring in the absence of that predictor (x = 0).

For a given predictor (say x1), the associated beta coefficient (b1) in the logistic regression function corresponds to the log of the odds ratio for that predictor.

If the odds ratio is 2, then the odds that the event occurs (event = 1) are two times higher when the predictor x is present (x = 1) versus x is absent (x = 0).

For example, the regression coefficient for glucose is 0.042. This indicate that one unit increase in the glucose concentration will increase the odds of being diabetes-positive by exp(0.042) 1.04 times.

From the logistic regression results, it can be noticed that some variables - triceps, insulin and age - are not statistically significant = they should be eliminated. 

Select manually the most significant:

```r
model &lt;- glm( diabetes ~ pregnant + glucose + pressure + mass + pedigree, 
                data = train_data, family = binomial)
```
---
#Assessing model accuracy

The *model accuracy* is measured as the proportion of observations that have been correctly classified. 

The *classification error* is defined as the proportion of observations that have been misclassified.

Proportion of correctly classified observations:

```r
mean(predicted.classes == test_data$diabetes)
```

```
## [1] 0.6923077
```

The classification prediction accuracy is about 76%, which is good. 

The misclassification error rate is 24%.

There are also several other metrics for evaluating the performance of a classification model 

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
