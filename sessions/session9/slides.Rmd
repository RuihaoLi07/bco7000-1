---
title: "BCO7000: Session 9"
subtitle: "⚔<br/>Basic modeling: exploratory data anlysis and linear regression"
author: "Dr Maria Prokofieva"
institute: "VU Business School"
date: "2021"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["shinobi", "ninjutsu"]
    seal: true
    self_contained: false
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false      
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE,   
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 4,
                      fig.width = 8,
                      fig.align = "center")

#install.packages("glue")
library(tidyverse)
library(skimr)
library(lubridate)
library(scales)
library(glue)
theme_set(theme_light())
```
# Modeling

## Exploratory Data Analysis (EDA)

use visualisation and transformation to explore your data in a systematic way

**Steps**:

- Generate questions about your data.

- Search for answers by visualising, transforming, and modelling your data.

- Use what you learn to refine your questions and/or generate new questions.

EDA is not a formal process with a strict set of rules:

- investigate every idea that occurs to you

EDA is an important part of any data analysis = investigate the quality of your data. 

---
#EDA

Two types of questions to ask to start:

What type of *variation* occurs within my variables?

What type of *covariation* occurs between my variables?

**Variation** is the tendency of the values of a variable to change from measurement to measurement. 

Examples: ?

The best way to understand that pattern is to visualise the distribution of the variable’s values.

---
#Variation in variables

Example of categorical variable

```{r}
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut), fill="navy")

#compare to

diamonds %>% 
  count(cut)
```
---
#Variation in variables

Example of continuous variable

A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. 

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)
```

---
# Variation in variables

Which values are the most common? Why?

Which values are rare? Why? Does that match your expectations?

Can you see any unusual patterns? What might explain them?

*For our example:*

Why are there more diamonds at whole carats and common fractions of carats?

Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?

Why are there no diamonds bigger than 3 carats?
---
#Pattern in data

The `faithful` data shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. 

Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between.

```{r}
ggplot(data = faithful, mapping = aes(x = eruptions)) + 
  geom_histogram(binwidth = 0.25)
```
---
#Outliers 

= observations that are unusual
- data points that don’t seem to fit the pattern. 
- can bedata entry errors or important new science. 
- can be difficult to see in a histogram 
*Hint*: look for wide limits on the x-axis.

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)
```

---
#Distribution of a continuous variable

A **boxplot** is a type of visual shorthand for a distribution of values that is popular among statisticians. 

Each boxplot consists of:

1. A box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the *interquartile range* (IQR). 

In the middle of the box is a line that displays the *median*, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.

- Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.

- A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.

---
# Boxplot:

![ ](img/eda-boxplot.png) 
---
# Boxplot:

```{r}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()
```

**Task**: identify quantiles, median, whiskers and outliers. What does this boxplot tell us about data?
---
# Boxplot:

```{r}

data(mpg)
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
```

**Task**: identify quantiles, median, whiskers and outliers. What does this boxplot tell us about data?

What does `reorder` do? Why do we use `FUN = median`?

---
```{r}

data(mpg)
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
```
Reordering factor based on their medians to make it look prettier!
---
# Hint:

use `coord_flip` for longer names


```{r}
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip()
```
---
# Task

Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.

Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)

How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?
---
# Outliers and missing values

Replacing the unusual values with missing values

```{r}
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))
```
Task: use `case_when` to complete the above
---
# Using `ggplot2

```{r}
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point()
```

or
---

```{r}
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point(na.rm = TRUE)
```

---
# Question

What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference?

What does `na.rm = TRUE` do in `mean()` and `sum()`?
---
# Covariation

= the tendency for the values of two or more variables to vary together in a related way. 

The best way to spot covariation is to visualise the relationship between two or more variables.

```{r}
ggplot(data = diamonds, mapping = aes(x = price)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```
---
#Covariation

To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination.

```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))
```

---

```{r}
ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)
```
---

# Models

If a systematic relationship exists between two variables it will appear as a pattern in the data:

Could this pattern be due to coincidence (i.e. random chance)?

How can you describe the relationship implied by the pattern?

How strong is the relationship implied by the pattern?

What other variables might affect the relationship?

Does the relationship change if you look at individual subgroups of the data?

```{r}
ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)
```
---
# Models

All models are wrong, but some are useful.

![ ](img/model.png) 

---
# Basic models

- regression models

- classification models
---
# Linear regression

is one of the simplest and most common supervised machine learning algorithms that data scientists use for predictive modeling.

Aim: to model a continuous variable `y` as a mathematical function of one or more `x` variable(s), so that we can use this regression model to predict the `y` when only the `x` is known:

y= b1 + b2*x1+error

Height = b1 +b2 age + b3 nationality + b4 parents + b5 height of neighbors

where, b1 is the intercept and b2 is the slope. 

Collectively, they are called regression coefficients, error is the error term, the part of y the regression model is unable to explain.

---
# Linear regression

Linear regression assumes that there exists a linear relationship between the response variable and the explanatory variables. 

This means that you can fit a line between the two (or more variables). 

![ ](img/lm.png) 
---
# Linear regression

![ ](img/lm-ex.png) 
you can calculate the height of a child if you know her age:

Height=a+Age∗b

In this case, “a” and “b” are called the intercept and the slope respectively. With the same example, “a” or the intercept, is the value from where you start measuring. Newborn babies with zero months are not zero centimeters necessarily; this is the function of the intercept. The slope measures the change of height with respect to the age in months. In general, for every month older the child is, his or her height will increase with “b”.

---
# Linear regression in R

A linear regression can be calculated in R with the command `lm`

```{r}

library(readxl)
ageandheight <- read_excel("ageandheight.xls", sheet = "Hoja2") #Upload the data
lmHeight = lm(height~age, data = ageandheight) #Create the linear regression
summary(lmHeight) #Review the results
```
---
# Coefficients

values of the intercept (“a” value) and the slope (“b” value) for the age. 

These “a” and “b” values plot a line between all the points of the data

e.g if there is a child that is 20.5 months old, a is 64.92 and b is 0.635, the model predicts (on average) that its height in centimeters is around 64.92 + (0.635 * 20.5) = 77.93 cm.
---
# Multiple linear regression

a regression takes into account two or more predictors to create the linear regression

Height = a + Age × b1 + (Number of Siblings} × b2

```{r}
lmHeight2 = lm(height~age + no_siblings, data = ageandheight) #Create a linear regression with two variables
summary(lmHeight2) #Review the results
```

---
# p-value

a p-value indicates whether or not you can reject or accept a hypothesis. The hypothesis, in this case, is that the predictor is not meaningful for your model.

The p-value for age is 4.34*e-10 or 0.000000000434. A very small value means that age is probably an excellent addition to your model.

The p-value for the number of siblings is 0.85. In other words, there’s 85% chance that this predictor is not meaningful for the regression.

A standard way to test if the predictors are not meaningful is looking if the p-values smaller than 0.05.

---
# Residuals

the differences between the real values and the predicted values. 

The straight line in the image above represents the predicted values. The red vertical line from the straight line to the observed data value is the residual.

Hint: keep the sum of the residuals is approximately zero or as low as possible
 
![ ](img/res.png) 
---
# How to test if your linear model has a good fit?


the coefficient of determination or R². This measure is defined by the proportion of the total variability explained by the regression model.

R2=Explained Variation of the model/ Total variation of the model

In general, for models that fit the data well, R² is near 1. 

Models that poorly fit the data have R² near 0. 

In the examples below, the first one has an R² of 0.02; this means that the model explains only 2% of the data variability. 

The second one has an R² of 0.99, and the model can explain 99% of the total variability.**

---
![ ](img/res2.png) 
---
# Don’t forget to look at the residuals!


